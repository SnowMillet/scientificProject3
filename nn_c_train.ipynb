{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 神经网络模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zj_c2_s0.1_b16_lr0.001_d0.5_e30\n",
      "models/model_zj_c2_s0.1_b16_lr0.001_d0.5_e30.pth\n",
      "True\n",
      "1\n",
      "GeForce MX150\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Para:\n",
    "    # tensor_board_log_dir = 'runs/exp0'\n",
    "    feature_column_start_name = 'ep_ratio_ttm'\n",
    "    feature_column_end_name = 'BR'\n",
    "\n",
    "    # 模型设置\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    classification = 2 # 2, 3\n",
    "\n",
    "    # 权重\n",
    "    cross_weight = list()\n",
    "    if classification == 3:\n",
    "        cross_weight = [1.0, 1.0 ,1.0]\n",
    "    elif classification == 2:\n",
    "        cross_weight = [1.0, 1.0]\n",
    "    elif classification == 5:\n",
    "        cross_weight = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "    batch_size = 16\n",
    "    lr = 1e-3\n",
    "    drop = 0.5\n",
    "    epochs = 30\n",
    "\n",
    "    # 数据集设置\n",
    "    month_in_sample = range(0, 1)\n",
    "    # month_test = range(36, 48)\n",
    "\n",
    "    percent_cv = 0.1 # 10% cross validation\n",
    "\n",
    "    data_path = 'data/zj_space_1d_rate_20d_12-21_pre'\n",
    "\n",
    "\n",
    "    seed = 2022\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    info_str0 = data_path[5:7]+'_'+'c'+str(classification)+'_s'+str(percent_cv)\n",
    "    info_str1 = '_b'+str(batch_size)+'_lr'+str(lr)+'_d'+str(drop)+'_e'+str(epochs)\n",
    "    info_str = info_str0 + info_str1\n",
    "\n",
    "    save_model_path = 'models/'+'model_'+info_str+'.pth'\n",
    "\n",
    "para = Para()\n",
    "print(para.info_str)\n",
    "print(para.save_model_path)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      return_bin order_book_id board_type  sector_code  month        date  \\\n0            0.0   600895.XSHG  MainBoard  Industrials      0  2012-01-04   \n1            0.0   600895.XSHG  MainBoard  Industrials      1  2012-01-05   \n2            0.0   600895.XSHG  MainBoard  Industrials      2  2012-01-06   \n3            0.0   600895.XSHG  MainBoard  Industrials      3  2012-01-09   \n4            0.0   600895.XSHG  MainBoard  Industrials      4  2012-01-10   \n...          ...           ...        ...          ...    ...         ...   \n2399         0.0   600895.XSHG  MainBoard  Industrials   2406  2021-11-29   \n2400         1.0   600895.XSHG  MainBoard  Industrials   2407  2021-11-30   \n2401         1.0   600895.XSHG  MainBoard  Industrials   2408  2021-12-01   \n2402         0.0   600895.XSHG  MainBoard  Industrials   2409  2021-12-02   \n2403         0.0   600895.XSHG  MainBoard  Industrials   2410  2021-12-03   \n\n      yield_rate  ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  ...     RSI10  \\\n0       0.668096     -1.603230     -1.051909      0.010194  ... -2.149004   \n1       0.908286     -1.597009     -1.091188      0.058377  ... -2.187196   \n2       0.988467     -1.599470     -1.075912      0.039320  ... -1.864218   \n3       0.786295     -1.609752     -1.008266     -0.040334  ... -0.608269   \n4       0.597582     -1.617382     -0.953712     -0.099439  ... -0.178138   \n...          ...           ...           ...           ...  ...       ...   \n2399    0.040620      2.505919     -0.723704     -0.316174  ... -1.192598   \n2400   -0.004882      2.473869     -0.710581     -0.326401  ... -0.555302   \n2401   -0.029767      2.462332     -0.705808     -0.330082  ... -0.587372   \n2402    0.010390      2.491292     -0.717739     -0.320842  ... -0.316660   \n2403    0.015187      2.485469     -0.715353     -0.322700  ... -0.878237   \n\n            SY    BIAS20     VOL30     VOL60    VOL120    VOLT20    VOLT60  \\\n0    -1.794150 -2.097427 -0.798000 -0.775429 -0.820660  0.210017 -0.349589   \n1    -1.794150 -2.235441 -0.797675 -0.774009 -0.820659  0.208850 -0.312678   \n2    -1.044691 -1.909176 -0.802157 -0.781049 -0.821995  0.173532 -0.282713   \n3    -1.044691 -1.111682 -0.793187 -0.780847 -0.820737  0.041498 -0.264281   \n4    -0.295232 -0.451633 -0.780389 -0.775671 -0.819612 -0.101613 -0.252861   \n...        ...       ...       ...       ...       ...       ...       ...   \n2399 -0.295232 -0.301873 -0.748341 -0.669583 -0.476146 -0.771154 -0.098022   \n2400 -0.295232 -0.191797 -0.747243 -0.674466 -0.512491 -0.764523 -0.082492   \n2401  0.454227 -0.144966 -0.746992 -0.676701 -0.549897 -0.763819 -0.068585   \n2402 -0.295232 -0.224451 -0.745291 -0.692358 -0.564154 -0.757408 -0.058939   \n2403  0.454227 -0.193108 -0.751198 -0.712014 -0.591063 -0.753987 -0.056085   \n\n            AR        BR  \n0    -1.549173 -1.406188  \n1    -1.602482 -1.492669  \n2    -1.718665 -1.616956  \n3    -1.538311 -1.347998  \n4    -1.442265 -1.457513  \n...        ...       ...  \n2399 -1.461414 -1.654446  \n2400 -1.568848 -1.513253  \n2401 -1.444480 -1.506378  \n2402 -1.300889 -1.402812  \n2403 -1.134342 -1.247305  \n\n[2404 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>return_bin</th>\n      <th>order_book_id</th>\n      <th>board_type</th>\n      <th>sector_code</th>\n      <th>month</th>\n      <th>date</th>\n      <th>yield_rate</th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>...</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>0</td>\n      <td>2012-01-04</td>\n      <td>0.668096</td>\n      <td>-1.603230</td>\n      <td>-1.051909</td>\n      <td>0.010194</td>\n      <td>...</td>\n      <td>-2.149004</td>\n      <td>-1.794150</td>\n      <td>-2.097427</td>\n      <td>-0.798000</td>\n      <td>-0.775429</td>\n      <td>-0.820660</td>\n      <td>0.210017</td>\n      <td>-0.349589</td>\n      <td>-1.549173</td>\n      <td>-1.406188</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>1</td>\n      <td>2012-01-05</td>\n      <td>0.908286</td>\n      <td>-1.597009</td>\n      <td>-1.091188</td>\n      <td>0.058377</td>\n      <td>...</td>\n      <td>-2.187196</td>\n      <td>-1.794150</td>\n      <td>-2.235441</td>\n      <td>-0.797675</td>\n      <td>-0.774009</td>\n      <td>-0.820659</td>\n      <td>0.208850</td>\n      <td>-0.312678</td>\n      <td>-1.602482</td>\n      <td>-1.492669</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>2</td>\n      <td>2012-01-06</td>\n      <td>0.988467</td>\n      <td>-1.599470</td>\n      <td>-1.075912</td>\n      <td>0.039320</td>\n      <td>...</td>\n      <td>-1.864218</td>\n      <td>-1.044691</td>\n      <td>-1.909176</td>\n      <td>-0.802157</td>\n      <td>-0.781049</td>\n      <td>-0.821995</td>\n      <td>0.173532</td>\n      <td>-0.282713</td>\n      <td>-1.718665</td>\n      <td>-1.616956</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>3</td>\n      <td>2012-01-09</td>\n      <td>0.786295</td>\n      <td>-1.609752</td>\n      <td>-1.008266</td>\n      <td>-0.040334</td>\n      <td>...</td>\n      <td>-0.608269</td>\n      <td>-1.044691</td>\n      <td>-1.111682</td>\n      <td>-0.793187</td>\n      <td>-0.780847</td>\n      <td>-0.820737</td>\n      <td>0.041498</td>\n      <td>-0.264281</td>\n      <td>-1.538311</td>\n      <td>-1.347998</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>4</td>\n      <td>2012-01-10</td>\n      <td>0.597582</td>\n      <td>-1.617382</td>\n      <td>-0.953712</td>\n      <td>-0.099439</td>\n      <td>...</td>\n      <td>-0.178138</td>\n      <td>-0.295232</td>\n      <td>-0.451633</td>\n      <td>-0.780389</td>\n      <td>-0.775671</td>\n      <td>-0.819612</td>\n      <td>-0.101613</td>\n      <td>-0.252861</td>\n      <td>-1.442265</td>\n      <td>-1.457513</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2399</th>\n      <td>0.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>2406</td>\n      <td>2021-11-29</td>\n      <td>0.040620</td>\n      <td>2.505919</td>\n      <td>-0.723704</td>\n      <td>-0.316174</td>\n      <td>...</td>\n      <td>-1.192598</td>\n      <td>-0.295232</td>\n      <td>-0.301873</td>\n      <td>-0.748341</td>\n      <td>-0.669583</td>\n      <td>-0.476146</td>\n      <td>-0.771154</td>\n      <td>-0.098022</td>\n      <td>-1.461414</td>\n      <td>-1.654446</td>\n    </tr>\n    <tr>\n      <th>2400</th>\n      <td>1.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>2407</td>\n      <td>2021-11-30</td>\n      <td>-0.004882</td>\n      <td>2.473869</td>\n      <td>-0.710581</td>\n      <td>-0.326401</td>\n      <td>...</td>\n      <td>-0.555302</td>\n      <td>-0.295232</td>\n      <td>-0.191797</td>\n      <td>-0.747243</td>\n      <td>-0.674466</td>\n      <td>-0.512491</td>\n      <td>-0.764523</td>\n      <td>-0.082492</td>\n      <td>-1.568848</td>\n      <td>-1.513253</td>\n    </tr>\n    <tr>\n      <th>2401</th>\n      <td>1.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>2408</td>\n      <td>2021-12-01</td>\n      <td>-0.029767</td>\n      <td>2.462332</td>\n      <td>-0.705808</td>\n      <td>-0.330082</td>\n      <td>...</td>\n      <td>-0.587372</td>\n      <td>0.454227</td>\n      <td>-0.144966</td>\n      <td>-0.746992</td>\n      <td>-0.676701</td>\n      <td>-0.549897</td>\n      <td>-0.763819</td>\n      <td>-0.068585</td>\n      <td>-1.444480</td>\n      <td>-1.506378</td>\n    </tr>\n    <tr>\n      <th>2402</th>\n      <td>0.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>2409</td>\n      <td>2021-12-02</td>\n      <td>0.010390</td>\n      <td>2.491292</td>\n      <td>-0.717739</td>\n      <td>-0.320842</td>\n      <td>...</td>\n      <td>-0.316660</td>\n      <td>-0.295232</td>\n      <td>-0.224451</td>\n      <td>-0.745291</td>\n      <td>-0.692358</td>\n      <td>-0.564154</td>\n      <td>-0.757408</td>\n      <td>-0.058939</td>\n      <td>-1.300889</td>\n      <td>-1.402812</td>\n    </tr>\n    <tr>\n      <th>2403</th>\n      <td>0.0</td>\n      <td>600895.XSHG</td>\n      <td>MainBoard</td>\n      <td>Industrials</td>\n      <td>2410</td>\n      <td>2021-12-03</td>\n      <td>0.015187</td>\n      <td>2.485469</td>\n      <td>-0.715353</td>\n      <td>-0.322700</td>\n      <td>...</td>\n      <td>-0.878237</td>\n      <td>0.454227</td>\n      <td>-0.193108</td>\n      <td>-0.751198</td>\n      <td>-0.712014</td>\n      <td>-0.591063</td>\n      <td>-0.753987</td>\n      <td>-0.056085</td>\n      <td>-1.134342</td>\n      <td>-1.247305</td>\n    </tr>\n  </tbody>\n</table>\n<p>2404 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_in_sample = None\n",
    "for i_month in para.month_in_sample:\n",
    "    file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "    data_curr_month = pd.read_csv(file_name)\n",
    "\n",
    "    data_curr_month = data_curr_month.dropna(axis=0)\n",
    "\n",
    "    data_curr_month.insert(loc=0, column='return_bin', value=np.nan)\n",
    "\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']>0, 'return_bin'] = 0\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']<=0, 'return_bin'] = 1\n",
    "\n",
    "    if i_month == para.month_in_sample[0]:\n",
    "        data_in_sample = data_curr_month\n",
    "    else:\n",
    "        data_in_sample = pd.concat([data_in_sample, data_curr_month])\n",
    "        # data_in_sample = data_in_sample.append(data_curr_month)\n",
    "\n",
    "data_in_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0        -1.603230     -1.051909      0.010194  -1.057518 -0.962171   \n1        -1.597009     -1.091188      0.058377  -1.091523 -0.997159   \n2        -1.599470     -1.075912      0.039320  -1.096094 -1.026104   \n3        -1.609752     -1.008266     -0.040334  -1.043153 -1.038215   \n4        -1.617382     -0.953712     -0.099439  -0.954356 -1.029381   \n...            ...           ...           ...        ...       ...   \n2158      0.486620      0.054286     -0.947774  -0.505349 -0.679579   \n2159      0.469203      0.073563     -0.953428  -0.445737 -0.636370   \n2160      0.503032      0.036386     -0.942447  -0.433055 -0.599157   \n2161      0.511967      0.026747     -0.939547  -0.428620 -0.568462   \n2162      0.531344      0.006093     -0.933258  -0.442163 -0.546731   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0     -0.496158 -2.149004 -1.794150 -2.097427 -0.798000 -0.775429 -0.820660   \n1     -0.497766 -2.187196 -1.794150 -2.235441 -0.797675 -0.774009 -0.820659   \n2     -0.416796 -1.864218 -1.044691 -1.909176 -0.802157 -0.781049 -0.821995   \n3     -0.191290 -0.608269 -1.044691 -1.111682 -0.793187 -0.780847 -0.820737   \n4      0.089322 -0.178138 -0.295232 -0.451633 -0.780389 -0.775671 -0.819612   \n...         ...       ...       ...       ...       ...       ...       ...   \n2158   0.486302  0.322967 -1.794150 -0.129465 -0.166240 -0.269208  0.852851   \n2159   0.549833  0.414804 -1.044691 -0.046674 -0.147391 -0.276099  0.821180   \n2160   0.469501  0.225628 -1.794150 -0.227087 -0.151157 -0.284739  0.807267   \n2161   0.382189  0.088561 -1.794150 -0.242801 -0.184586 -0.293756  0.740537   \n2162   0.262095  0.102907 -1.794150 -0.306581 -0.202539 -0.299831  0.698498   \n\n        VOLT20    VOLT60        AR        BR  \n0     0.210017 -0.349589 -1.549173 -1.406188  \n1     0.208850 -0.312678 -1.602482 -1.492669  \n2     0.173532 -0.282713 -1.718665 -1.616956  \n3     0.041498 -0.264281 -1.538311 -1.347998  \n4    -0.101613 -0.252861 -1.442265 -1.457513  \n...        ...       ...       ...       ...  \n2158  0.059265 -0.116384 -1.193304 -0.821015  \n2159  0.037877 -0.176032 -1.093606 -0.641653  \n2160  0.031992 -0.218353 -1.194527 -0.735829  \n2161  0.004905 -0.254286 -1.175696 -0.642815  \n2162 -0.047552 -0.294385 -1.205408 -0.620915  \n\n[2163 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.603230</td>\n      <td>-1.051909</td>\n      <td>0.010194</td>\n      <td>-1.057518</td>\n      <td>-0.962171</td>\n      <td>-0.496158</td>\n      <td>-2.149004</td>\n      <td>-1.794150</td>\n      <td>-2.097427</td>\n      <td>-0.798000</td>\n      <td>-0.775429</td>\n      <td>-0.820660</td>\n      <td>0.210017</td>\n      <td>-0.349589</td>\n      <td>-1.549173</td>\n      <td>-1.406188</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.597009</td>\n      <td>-1.091188</td>\n      <td>0.058377</td>\n      <td>-1.091523</td>\n      <td>-0.997159</td>\n      <td>-0.497766</td>\n      <td>-2.187196</td>\n      <td>-1.794150</td>\n      <td>-2.235441</td>\n      <td>-0.797675</td>\n      <td>-0.774009</td>\n      <td>-0.820659</td>\n      <td>0.208850</td>\n      <td>-0.312678</td>\n      <td>-1.602482</td>\n      <td>-1.492669</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.599470</td>\n      <td>-1.075912</td>\n      <td>0.039320</td>\n      <td>-1.096094</td>\n      <td>-1.026104</td>\n      <td>-0.416796</td>\n      <td>-1.864218</td>\n      <td>-1.044691</td>\n      <td>-1.909176</td>\n      <td>-0.802157</td>\n      <td>-0.781049</td>\n      <td>-0.821995</td>\n      <td>0.173532</td>\n      <td>-0.282713</td>\n      <td>-1.718665</td>\n      <td>-1.616956</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.609752</td>\n      <td>-1.008266</td>\n      <td>-0.040334</td>\n      <td>-1.043153</td>\n      <td>-1.038215</td>\n      <td>-0.191290</td>\n      <td>-0.608269</td>\n      <td>-1.044691</td>\n      <td>-1.111682</td>\n      <td>-0.793187</td>\n      <td>-0.780847</td>\n      <td>-0.820737</td>\n      <td>0.041498</td>\n      <td>-0.264281</td>\n      <td>-1.538311</td>\n      <td>-1.347998</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.617382</td>\n      <td>-0.953712</td>\n      <td>-0.099439</td>\n      <td>-0.954356</td>\n      <td>-1.029381</td>\n      <td>0.089322</td>\n      <td>-0.178138</td>\n      <td>-0.295232</td>\n      <td>-0.451633</td>\n      <td>-0.780389</td>\n      <td>-0.775671</td>\n      <td>-0.819612</td>\n      <td>-0.101613</td>\n      <td>-0.252861</td>\n      <td>-1.442265</td>\n      <td>-1.457513</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2158</th>\n      <td>0.486620</td>\n      <td>0.054286</td>\n      <td>-0.947774</td>\n      <td>-0.505349</td>\n      <td>-0.679579</td>\n      <td>0.486302</td>\n      <td>0.322967</td>\n      <td>-1.794150</td>\n      <td>-0.129465</td>\n      <td>-0.166240</td>\n      <td>-0.269208</td>\n      <td>0.852851</td>\n      <td>0.059265</td>\n      <td>-0.116384</td>\n      <td>-1.193304</td>\n      <td>-0.821015</td>\n    </tr>\n    <tr>\n      <th>2159</th>\n      <td>0.469203</td>\n      <td>0.073563</td>\n      <td>-0.953428</td>\n      <td>-0.445737</td>\n      <td>-0.636370</td>\n      <td>0.549833</td>\n      <td>0.414804</td>\n      <td>-1.044691</td>\n      <td>-0.046674</td>\n      <td>-0.147391</td>\n      <td>-0.276099</td>\n      <td>0.821180</td>\n      <td>0.037877</td>\n      <td>-0.176032</td>\n      <td>-1.093606</td>\n      <td>-0.641653</td>\n    </tr>\n    <tr>\n      <th>2160</th>\n      <td>0.503032</td>\n      <td>0.036386</td>\n      <td>-0.942447</td>\n      <td>-0.433055</td>\n      <td>-0.599157</td>\n      <td>0.469501</td>\n      <td>0.225628</td>\n      <td>-1.794150</td>\n      <td>-0.227087</td>\n      <td>-0.151157</td>\n      <td>-0.284739</td>\n      <td>0.807267</td>\n      <td>0.031992</td>\n      <td>-0.218353</td>\n      <td>-1.194527</td>\n      <td>-0.735829</td>\n    </tr>\n    <tr>\n      <th>2161</th>\n      <td>0.511967</td>\n      <td>0.026747</td>\n      <td>-0.939547</td>\n      <td>-0.428620</td>\n      <td>-0.568462</td>\n      <td>0.382189</td>\n      <td>0.088561</td>\n      <td>-1.794150</td>\n      <td>-0.242801</td>\n      <td>-0.184586</td>\n      <td>-0.293756</td>\n      <td>0.740537</td>\n      <td>0.004905</td>\n      <td>-0.254286</td>\n      <td>-1.175696</td>\n      <td>-0.642815</td>\n    </tr>\n    <tr>\n      <th>2162</th>\n      <td>0.531344</td>\n      <td>0.006093</td>\n      <td>-0.933258</td>\n      <td>-0.442163</td>\n      <td>-0.546731</td>\n      <td>0.262095</td>\n      <td>0.102907</td>\n      <td>-1.794150</td>\n      <td>-0.306581</td>\n      <td>-0.202539</td>\n      <td>-0.299831</td>\n      <td>0.698498</td>\n      <td>-0.047552</td>\n      <td>-0.294385</td>\n      <td>-1.205408</td>\n      <td>-0.620915</td>\n    </tr>\n  </tbody>\n</table>\n<p>2163 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in_sample = data_in_sample.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "y_in_sample = data_in_sample.loc[:, 'return_bin']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False) # True, random_state=para.seed)\n",
    "# X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0       0.0\n1       0.0\n2       0.0\n3       0.0\n4       0.0\n       ... \n2158    1.0\n2159    1.0\n2160    1.0\n2161    1.0\n2162    1.0\nName: return_bin, Length: 2163, dtype: float64"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0        -1.603230     -1.051909      0.010194  -1.057518 -0.962171   \n1        -1.597009     -1.091188      0.058377  -1.091523 -0.997159   \n2        -1.599470     -1.075912      0.039320  -1.096094 -1.026104   \n3        -1.609752     -1.008266     -0.040334  -1.043153 -1.038215   \n4        -1.617382     -0.953712     -0.099439  -0.954356 -1.029381   \n...            ...           ...           ...        ...       ...   \n2158      0.486620      0.054286     -0.947774  -0.505349 -0.679579   \n2159      0.469203      0.073563     -0.953428  -0.445737 -0.636370   \n2160      0.503032      0.036386     -0.942447  -0.433055 -0.599157   \n2161      0.511967      0.026747     -0.939547  -0.428620 -0.568462   \n2162      0.531344      0.006093     -0.933258  -0.442163 -0.546731   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0     -0.496158 -2.149004 -1.794150 -2.097427 -0.798000 -0.775429 -0.820660   \n1     -0.497766 -2.187196 -1.794150 -2.235441 -0.797675 -0.774009 -0.820659   \n2     -0.416796 -1.864218 -1.044691 -1.909176 -0.802157 -0.781049 -0.821995   \n3     -0.191290 -0.608269 -1.044691 -1.111682 -0.793187 -0.780847 -0.820737   \n4      0.089322 -0.178138 -0.295232 -0.451633 -0.780389 -0.775671 -0.819612   \n...         ...       ...       ...       ...       ...       ...       ...   \n2158   0.486302  0.322967 -1.794150 -0.129465 -0.166240 -0.269208  0.852851   \n2159   0.549833  0.414804 -1.044691 -0.046674 -0.147391 -0.276099  0.821180   \n2160   0.469501  0.225628 -1.794150 -0.227087 -0.151157 -0.284739  0.807267   \n2161   0.382189  0.088561 -1.794150 -0.242801 -0.184586 -0.293756  0.740537   \n2162   0.262095  0.102907 -1.794150 -0.306581 -0.202539 -0.299831  0.698498   \n\n        VOLT20    VOLT60        AR        BR  return_bin  \n0     0.210017 -0.349589 -1.549173 -1.406188         0.0  \n1     0.208850 -0.312678 -1.602482 -1.492669         0.0  \n2     0.173532 -0.282713 -1.718665 -1.616956         0.0  \n3     0.041498 -0.264281 -1.538311 -1.347998         0.0  \n4    -0.101613 -0.252861 -1.442265 -1.457513         0.0  \n...        ...       ...       ...       ...         ...  \n2158  0.059265 -0.116384 -1.193304 -0.821015         1.0  \n2159  0.037877 -0.176032 -1.093606 -0.641653         1.0  \n2160  0.031992 -0.218353 -1.194527 -0.735829         1.0  \n2161  0.004905 -0.254286 -1.175696 -0.642815         1.0  \n2162 -0.047552 -0.294385 -1.205408 -0.620915         1.0  \n\n[2163 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n      <th>return_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.603230</td>\n      <td>-1.051909</td>\n      <td>0.010194</td>\n      <td>-1.057518</td>\n      <td>-0.962171</td>\n      <td>-0.496158</td>\n      <td>-2.149004</td>\n      <td>-1.794150</td>\n      <td>-2.097427</td>\n      <td>-0.798000</td>\n      <td>-0.775429</td>\n      <td>-0.820660</td>\n      <td>0.210017</td>\n      <td>-0.349589</td>\n      <td>-1.549173</td>\n      <td>-1.406188</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.597009</td>\n      <td>-1.091188</td>\n      <td>0.058377</td>\n      <td>-1.091523</td>\n      <td>-0.997159</td>\n      <td>-0.497766</td>\n      <td>-2.187196</td>\n      <td>-1.794150</td>\n      <td>-2.235441</td>\n      <td>-0.797675</td>\n      <td>-0.774009</td>\n      <td>-0.820659</td>\n      <td>0.208850</td>\n      <td>-0.312678</td>\n      <td>-1.602482</td>\n      <td>-1.492669</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.599470</td>\n      <td>-1.075912</td>\n      <td>0.039320</td>\n      <td>-1.096094</td>\n      <td>-1.026104</td>\n      <td>-0.416796</td>\n      <td>-1.864218</td>\n      <td>-1.044691</td>\n      <td>-1.909176</td>\n      <td>-0.802157</td>\n      <td>-0.781049</td>\n      <td>-0.821995</td>\n      <td>0.173532</td>\n      <td>-0.282713</td>\n      <td>-1.718665</td>\n      <td>-1.616956</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.609752</td>\n      <td>-1.008266</td>\n      <td>-0.040334</td>\n      <td>-1.043153</td>\n      <td>-1.038215</td>\n      <td>-0.191290</td>\n      <td>-0.608269</td>\n      <td>-1.044691</td>\n      <td>-1.111682</td>\n      <td>-0.793187</td>\n      <td>-0.780847</td>\n      <td>-0.820737</td>\n      <td>0.041498</td>\n      <td>-0.264281</td>\n      <td>-1.538311</td>\n      <td>-1.347998</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.617382</td>\n      <td>-0.953712</td>\n      <td>-0.099439</td>\n      <td>-0.954356</td>\n      <td>-1.029381</td>\n      <td>0.089322</td>\n      <td>-0.178138</td>\n      <td>-0.295232</td>\n      <td>-0.451633</td>\n      <td>-0.780389</td>\n      <td>-0.775671</td>\n      <td>-0.819612</td>\n      <td>-0.101613</td>\n      <td>-0.252861</td>\n      <td>-1.442265</td>\n      <td>-1.457513</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2158</th>\n      <td>0.486620</td>\n      <td>0.054286</td>\n      <td>-0.947774</td>\n      <td>-0.505349</td>\n      <td>-0.679579</td>\n      <td>0.486302</td>\n      <td>0.322967</td>\n      <td>-1.794150</td>\n      <td>-0.129465</td>\n      <td>-0.166240</td>\n      <td>-0.269208</td>\n      <td>0.852851</td>\n      <td>0.059265</td>\n      <td>-0.116384</td>\n      <td>-1.193304</td>\n      <td>-0.821015</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2159</th>\n      <td>0.469203</td>\n      <td>0.073563</td>\n      <td>-0.953428</td>\n      <td>-0.445737</td>\n      <td>-0.636370</td>\n      <td>0.549833</td>\n      <td>0.414804</td>\n      <td>-1.044691</td>\n      <td>-0.046674</td>\n      <td>-0.147391</td>\n      <td>-0.276099</td>\n      <td>0.821180</td>\n      <td>0.037877</td>\n      <td>-0.176032</td>\n      <td>-1.093606</td>\n      <td>-0.641653</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2160</th>\n      <td>0.503032</td>\n      <td>0.036386</td>\n      <td>-0.942447</td>\n      <td>-0.433055</td>\n      <td>-0.599157</td>\n      <td>0.469501</td>\n      <td>0.225628</td>\n      <td>-1.794150</td>\n      <td>-0.227087</td>\n      <td>-0.151157</td>\n      <td>-0.284739</td>\n      <td>0.807267</td>\n      <td>0.031992</td>\n      <td>-0.218353</td>\n      <td>-1.194527</td>\n      <td>-0.735829</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2161</th>\n      <td>0.511967</td>\n      <td>0.026747</td>\n      <td>-0.939547</td>\n      <td>-0.428620</td>\n      <td>-0.568462</td>\n      <td>0.382189</td>\n      <td>0.088561</td>\n      <td>-1.794150</td>\n      <td>-0.242801</td>\n      <td>-0.184586</td>\n      <td>-0.293756</td>\n      <td>0.740537</td>\n      <td>0.004905</td>\n      <td>-0.254286</td>\n      <td>-1.175696</td>\n      <td>-0.642815</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2162</th>\n      <td>0.531344</td>\n      <td>0.006093</td>\n      <td>-0.933258</td>\n      <td>-0.442163</td>\n      <td>-0.546731</td>\n      <td>0.262095</td>\n      <td>0.102907</td>\n      <td>-1.794150</td>\n      <td>-0.306581</td>\n      <td>-0.202539</td>\n      <td>-0.299831</td>\n      <td>0.698498</td>\n      <td>-0.047552</td>\n      <td>-0.294385</td>\n      <td>-1.205408</td>\n      <td>-0.620915</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2163 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_cv = pd.concat([X_cv, y_cv], axis=1)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "X_train_ndarray = X_train.values\n",
    "y_train_ndarray = y_train.values\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_ndarray).type(torch.FloatTensor), torch.from_numpy(y_train_ndarray).type(torch.LongTensor))\n",
    "\n",
    "# for X_train_temp, y_train_temp in train_dataset:\n",
    "#     print(X_train_temp, y_train_temp)\n",
    "#     print(X_train_temp.dtype, y_train_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_cv_ndarray = X_cv.values\n",
    "y_cv_ndarray = y_cv.values\n",
    "\n",
    "cv_dataset = TensorDataset(torch.from_numpy(X_cv_ndarray).type(torch.FloatTensor), torch.from_numpy(y_cv.values).type(torch.LongTensor))\n",
    "\n",
    "# for X_cv_temp, y_cv_temp in cv_dataset:\n",
    "#     print(X_cv_temp, y_cv_temp)\n",
    "#     print(X_cv_temp.dtype, y_cv_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cv_dataloader = DataLoader(\n",
    "    dataset=cv_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_test = None\n",
    "# for i_month in para.month_test:\n",
    "#\n",
    "#     file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "#     data_curr_month = pd.read_csv(file_name)\n",
    "#\n",
    "#     data_curr_month = data_curr_month.dropna(axis=0)\n",
    "#\n",
    "#     data_curr_month = label_data(data=data_curr_month, percent_select=para.percent_select)\n",
    "#\n",
    "#     if i_month == para.month_test[0]:\n",
    "#         data_test = data_curr_month\n",
    "#     else:\n",
    "#         data_test = pd.concat([data_test, data_curr_month])\n",
    "#         # data_test = data_test.append(data_curr_month)\n",
    "#\n",
    "# X_test = data_test.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "# y_test = data_test.loc[:, 'return_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset\n",
    "# import torch\n",
    "#\n",
    "#\n",
    "# X_test_ndarray = X_test.values\n",
    "# y_test_ndarray = y_test.values\n",
    "#\n",
    "# test_dataset = TensorDataset(torch.from_numpy(X_test_ndarray).type(torch.FloatTensor), torch.from_numpy(y_test_ndarray).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "#\n",
    "#\n",
    "# test_dataloader = DataLoader(\n",
    "#     dataset=test_dataset,\n",
    "#     batch_size=para.batch_size,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from my_utils.model_class import MLP\n",
    "\n",
    "model = MLP(in_nums=len(X_train.columns), out_nums=para.classification, drop_p=para.drop)\n",
    "# to device\n",
    "model = model.to(device=para.device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    train_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # to device\n",
    "        X = X.to(device=para.device)\n",
    "        y = y.to(device=para.device)\n",
    "\n",
    "        # compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        train_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # metric on current batch\n",
    "        train_precision(pred.argmax(1), y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = train_precision.compute()\n",
    "    print(\"Precision of every train dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, train_loss, total_precision\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    test_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            # to device\n",
    "            X = X.to(device=para.device)\n",
    "            y = y.to(device=para.device)\n",
    "\n",
    "            # compute prediction and loss\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            # metric on current batch\n",
    "            test_precision(pred.argmax(1), y)\n",
    "\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = test_precision.compute()\n",
    "    print(\"Precision of every test dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, test_loss, total_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_df_to_dataloader(df: pd.DataFrame, select: int) -> DataLoader:\n",
    "\n",
    "    df = df[df['return_bin'] == select]\n",
    "\n",
    "    df_dataset = TensorDataset(\n",
    "        torch.from_numpy(df.loc[:, para.feature_column_start_name: para.feature_column_end_name].values).type(torch.FloatTensor),\n",
    "        torch.from_numpy(df.loc[:, 'return_bin'].values).type(torch.LongTensor))\n",
    "\n",
    "    df_dataloader = DataLoader(\n",
    "        dataset=df_dataset,\n",
    "        batch_size=para.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return df_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp2_dataloader = select_df_to_dataloader(df=data_cv, select=2)\n",
    "temp1_dataloader = select_df_to_dataloader(df=data_cv, select=1)\n",
    "temp0_dataloader = select_df_to_dataloader(df=data_cv, select=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 54.2%, Avg loss: 0.691567 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.3852, 0.5751], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.651040 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 2.387602s\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.681130 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5132, 0.5855], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.641971 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 4.102203s\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 58.6%, Avg loss: 0.677093 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5789, 0.5864], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.653161 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 5.691109s\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.1%, Avg loss: 0.673071 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5893, 0.5914], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.628431 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 6.999462s\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.3%, Avg loss: 0.665617 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6047, 0.6027], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.648107 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 8.371765s\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.8%, Avg loss: 0.663503 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6022, 0.6088], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.649343 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 9.584501s\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.8%, Avg loss: 0.653715 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6149, 0.6182], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.609152 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 10.952815s\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.7%, Avg loss: 0.652013 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5922, 0.6231], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.609812 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 12.478670s\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.4%, Avg loss: 0.650182 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6166, 0.6252], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.650616 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 14.111079s\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.5%, Avg loss: 0.640313 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6200, 0.6395], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.605538 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 15.388046s\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.8%, Avg loss: 0.639201 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6380, 0.6386], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.610812 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 16.556531s\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.7%, Avg loss: 0.637114 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6358, 0.6509], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.659208 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 17.797668s\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.0%, Avg loss: 0.638947 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6291, 0.6436], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.661040 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 19.152364s\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 65.3%, Avg loss: 0.635424 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6539, 0.6525], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.605606 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 20.634558s\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 66.2%, Avg loss: 0.627070 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6530, 0.6646], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.610939 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 21.885734s\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.2%, Avg loss: 0.635085 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6267, 0.6463], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.612100 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 23.439579s\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 66.0%, Avg loss: 0.626023 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6516, 0.6625], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.619234 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 24.950233s\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.7%, Avg loss: 0.617900 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6840, 0.6743], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.619719 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 26.448571s\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 66.5%, Avg loss: 0.627239 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6552, 0.6690], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.616112 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 27.804998s\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 66.8%, Avg loss: 0.620899 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6667, 0.6685], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.619354 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 28.974060s\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.6%, Avg loss: 0.617798 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6789, 0.6755], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.669930 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 30.211486s\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 66.8%, Avg loss: 0.616151 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6565, 0.6718], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.678557 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 31.351531s\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.4%, Avg loss: 0.624709 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6672, 0.6767], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.613339 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 32.700973s\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.2%, Avg loss: 0.615635 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6690, 0.6728], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.619368 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 34.022560s\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.5%, Avg loss: 0.618517 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6656, 0.6780], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.621439 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 35.227893s\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.3%, Avg loss: 0.608597 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6639, 0.6760], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.618349 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 37.154344s\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.8%, Avg loss: 0.608022 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6745, 0.6796], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.615819 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 38.362450s\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 68.0%, Avg loss: 0.608705 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6693, 0.6838], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.611794 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 39.671518s\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 68.5%, Avg loss: 0.614619 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6795, 0.6874], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.603923 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 40.791520s\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.8%, Avg loss: 0.611624 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6699, 0.6815], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.615236 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.6929], device='cuda:0')\n",
      "\n",
      "Time cost = 42.131478s\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 计时\n",
    "time_start = time.time()\n",
    "\n",
    "# writer = SummaryWriter(para.tensor_board_log_dir)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# to device\n",
    "loss_fn = loss_fn.to(device=para.device)\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=para.lr)\n",
    "\n",
    "epochs = para.epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    model.train()\n",
    "    accuracy_train, loss_train, precision_train = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    model.eval()\n",
    "    accuracy_cv, loss_cv, precision_cv = test_loop(cv_dataloader, model, loss_fn)\n",
    "\n",
    "    # accuracy2 = test_loop(temp2_dataloader, model, loss_fn)\n",
    "    # print('#')\n",
    "    # accuracy1 = test_loop(temp1_dataloader, model, loss_fn)\n",
    "    # accuracy0 = test_loop(temp0_dataloader, model, loss_fn)\n",
    "\n",
    "    # 写入 tensorboard\n",
    "    if para.classification == 2:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 3:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 5:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2],\n",
    "                               'precision3': precision_cv[3],\n",
    "                               'precision4': precision_cv[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2],\n",
    "                               'precision3': precision_train[3],\n",
    "                               'precision4': precision_train[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/cv',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_cv},\n",
    "                       global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/train',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_train},\n",
    "                       global_step=t)\n",
    "    writer.flush()\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('Time cost = %fs' % (time_end - time_start))\n",
    "    print()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 保存模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish save model!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), para.save_model_path)\n",
    "\n",
    "print('Finish save model!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # captum\n",
    "# from captum.attr import IntegratedGradients\n",
    "#\n",
    "# ig = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp = cv_dataloader.dataset.tensors[0]\n",
    "# temp.requires_grad_()\n",
    "# attr, delta = ig.attribute(temp,target=1, return_convergence_delta=True)\n",
    "# attr = attr.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Helper method to print importances and visualize distribution\n",
    "# def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "#     print(title)\n",
    "#     for i in range(len(feature_names)):\n",
    "#         print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "#     y_pos = (np.arange(len(feature_names)))\n",
    "#     if plot:\n",
    "#         plt.figure(figsize=(20,6))\n",
    "#         plt.barh(y_pos, importances, align='center')\n",
    "#         plt.yticks(y_pos, feature_names)\n",
    "#         plt.ylabel(axis_title)\n",
    "#         plt.grid(axis='y')\n",
    "#         plt.title(title)\n",
    "# visualize_importances(feature_names=X_cv.columns.values.tolist(), importances=np.mean(attr, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_cv.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.Tensor(\n",
    "#     [[-0.0441,  0.0773],\n",
    "#     [-0.0781, -0.1772],\n",
    "#     [-0.1319, -0.0432],\n",
    "#     [-0.0714, -0.1261],\n",
    "#     [-0.0806, -0.1370],\n",
    "#     [-0.1730, -0.1472],\n",
    "#     [-0.0350, -0.0507],\n",
    "#     [-0.1149, -0.2248]])\n",
    "# # input = input.reshape(-1,4)\n",
    "# target = torch.Tensor([0, 1, 1, 0, 0, 0, 0, 0]).type(torch.LongTensor)\n",
    "# print(input.dtype)\n",
    "# print(target.dtype)\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Example of target with class indices\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.Tensor([1,4,1]).type(torch.LongTensor)\n",
    "# output = loss(input, target)\n",
    "# print(input,target,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# input = torch.Tensor([0.5, 0.4, 0.3])\n",
    "# target = torch.Tensor([0])\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}