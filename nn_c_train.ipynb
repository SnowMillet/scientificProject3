{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 神经网络模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt_c2_s0.1_b16_lr0.001_d0.5_e60\n",
      "models/model_mt_c2_s0.1_b16_lr0.001_d0.5_e60.pth\n",
      "True\n",
      "1\n",
      "GeForce MX150\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Para:\n",
    "    # tensor_board_log_dir = 'runs/exp0'\n",
    "    feature_column_start_name = 'ep_ratio_ttm'\n",
    "    feature_column_end_name = 'BR'\n",
    "\n",
    "    # 模型设置\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    classification = 2 # 2, 3\n",
    "\n",
    "    # 权重\n",
    "    cross_weight = list()\n",
    "    if classification == 3:\n",
    "        cross_weight = [1.0, 1.0 ,1.0]\n",
    "    elif classification == 2:\n",
    "        cross_weight = [1.0, 1.0]\n",
    "    elif classification == 5:\n",
    "        cross_weight = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "    batch_size = 16\n",
    "    lr = 1e-3\n",
    "    drop = 0.5\n",
    "    epochs = 60\n",
    "\n",
    "    # 数据集设置\n",
    "    month_in_sample = range(0, 1)\n",
    "    # month_test = range(36, 48)\n",
    "\n",
    "    percent_cv = 0.1 # 10% cross validation\n",
    "\n",
    "    data_path = 'data/mt_space_1d_rate_20d_12-21_pre'\n",
    "\n",
    "\n",
    "    seed = 2022\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    info_str0 = data_path[5:7]+'_'+'c'+str(classification)+'_s'+str(percent_cv)\n",
    "    info_str1 = '_b'+str(batch_size)+'_lr'+str(lr)+'_d'+str(drop)+'_e'+str(epochs)\n",
    "    info_str = info_str0 + info_str1\n",
    "\n",
    "    save_model_path = 'models/'+'model_'+info_str+'.pth'\n",
    "\n",
    "para = Para()\n",
    "print(para.info_str)\n",
    "print(para.save_model_path)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      return_bin order_book_id board_type      sector_code  month        date  \\\n0            1.0   600519.XSHG  MainBoard  ConsumerStaples      0  2012-01-04   \n1            0.0   600519.XSHG  MainBoard  ConsumerStaples      1  2012-01-05   \n2            1.0   600519.XSHG  MainBoard  ConsumerStaples      2  2012-01-06   \n3            0.0   600519.XSHG  MainBoard  ConsumerStaples      3  2012-01-09   \n4            1.0   600519.XSHG  MainBoard  ConsumerStaples      4  2012-01-10   \n...          ...           ...        ...              ...    ...         ...   \n2400         0.0   600519.XSHG  MainBoard  ConsumerStaples   2406  2021-11-29   \n2401         0.0   600519.XSHG  MainBoard  ConsumerStaples   2407  2021-11-30   \n2402         0.0   600519.XSHG  MainBoard  ConsumerStaples   2408  2021-12-01   \n2403         0.0   600519.XSHG  MainBoard  ConsumerStaples   2409  2021-12-02   \n2404         0.0   600519.XSHG  MainBoard  ConsumerStaples   2410  2021-12-03   \n\n      yield_rate  ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  ...     RSI10  \\\n0      -0.121926     -0.327793      0.119714     -0.281492  ... -1.516255   \n1       0.164856     -0.308310      0.092927     -0.260658  ... -1.570016   \n2      -0.071459     -0.340148      0.137024     -0.294705  ... -1.041819   \n3       0.020061     -0.352324      0.154334     -0.307724  ... -1.155946   \n4      -0.346104     -0.407505      0.236085     -0.366732  ... -0.648100   \n...          ...           ...           ...           ...  ...       ...   \n2400    0.534437     -1.136781      1.505894     -1.190443  ...  1.818819   \n2401    0.915285     -1.112226      1.404070     -1.165418  ...  0.873176   \n2402    0.328554     -1.113259      1.408238     -1.166470  ...  1.054461   \n2403    0.522385     -1.113728      1.410134     -1.166948  ...  1.100556   \n2404    0.143471     -1.130964      1.481260     -1.184515  ...  1.107237   \n\n            SY    BIAS20     VOL30     VOL60    VOL120    VOLT20    VOLT60  \\\n0     0.287582 -1.499037  0.243383 -0.108178 -0.064657 -0.714641 -0.765841   \n1     0.287582 -1.622616  0.227465 -0.093272 -0.060556 -0.695599 -0.763324   \n2     0.287582 -1.152234  0.209195 -0.100443 -0.063556 -0.697842 -0.760783   \n3     0.287582 -0.927281  0.208047 -0.097007 -0.059870 -0.700386 -0.759030   \n4     1.031276 -0.200881  0.205168 -0.055170 -0.046756 -0.704483 -0.759990   \n...        ...       ...       ...       ...       ...       ...       ...   \n2400  1.031276  1.512096 -0.824439 -0.534328 -0.416452  2.195698  1.699644   \n2401  1.031276  0.815918 -0.830065 -0.533581 -0.409498  2.318976  1.678364   \n2402  1.774970  0.741040 -0.834063 -0.556413 -0.412365  2.346959  1.643454   \n2403  1.774970  0.675672 -0.838373 -0.577372 -0.415137  2.417174  1.576462   \n2404  1.774970  1.013093 -0.838613 -0.614772 -0.421402  2.582774  1.574862   \n\n            AR        BR  \n0    -1.286900 -1.005713  \n1    -1.420785 -1.197929  \n2    -1.311010 -1.105942  \n3    -1.307988 -1.151213  \n4    -1.042536 -1.143190  \n...        ...       ...  \n2400  0.822318  0.443818  \n2401  0.533745  0.381303  \n2402  0.429537  0.580646  \n2403  0.646957  0.947160  \n2404  1.046325  1.202162  \n\n[2405 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>return_bin</th>\n      <th>order_book_id</th>\n      <th>board_type</th>\n      <th>sector_code</th>\n      <th>month</th>\n      <th>date</th>\n      <th>yield_rate</th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>...</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>0</td>\n      <td>2012-01-04</td>\n      <td>-0.121926</td>\n      <td>-0.327793</td>\n      <td>0.119714</td>\n      <td>-0.281492</td>\n      <td>...</td>\n      <td>-1.516255</td>\n      <td>0.287582</td>\n      <td>-1.499037</td>\n      <td>0.243383</td>\n      <td>-0.108178</td>\n      <td>-0.064657</td>\n      <td>-0.714641</td>\n      <td>-0.765841</td>\n      <td>-1.286900</td>\n      <td>-1.005713</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>1</td>\n      <td>2012-01-05</td>\n      <td>0.164856</td>\n      <td>-0.308310</td>\n      <td>0.092927</td>\n      <td>-0.260658</td>\n      <td>...</td>\n      <td>-1.570016</td>\n      <td>0.287582</td>\n      <td>-1.622616</td>\n      <td>0.227465</td>\n      <td>-0.093272</td>\n      <td>-0.060556</td>\n      <td>-0.695599</td>\n      <td>-0.763324</td>\n      <td>-1.420785</td>\n      <td>-1.197929</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>2</td>\n      <td>2012-01-06</td>\n      <td>-0.071459</td>\n      <td>-0.340148</td>\n      <td>0.137024</td>\n      <td>-0.294705</td>\n      <td>...</td>\n      <td>-1.041819</td>\n      <td>0.287582</td>\n      <td>-1.152234</td>\n      <td>0.209195</td>\n      <td>-0.100443</td>\n      <td>-0.063556</td>\n      <td>-0.697842</td>\n      <td>-0.760783</td>\n      <td>-1.311010</td>\n      <td>-1.105942</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>3</td>\n      <td>2012-01-09</td>\n      <td>0.020061</td>\n      <td>-0.352324</td>\n      <td>0.154334</td>\n      <td>-0.307724</td>\n      <td>...</td>\n      <td>-1.155946</td>\n      <td>0.287582</td>\n      <td>-0.927281</td>\n      <td>0.208047</td>\n      <td>-0.097007</td>\n      <td>-0.059870</td>\n      <td>-0.700386</td>\n      <td>-0.759030</td>\n      <td>-1.307988</td>\n      <td>-1.151213</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>4</td>\n      <td>2012-01-10</td>\n      <td>-0.346104</td>\n      <td>-0.407505</td>\n      <td>0.236085</td>\n      <td>-0.366732</td>\n      <td>...</td>\n      <td>-0.648100</td>\n      <td>1.031276</td>\n      <td>-0.200881</td>\n      <td>0.205168</td>\n      <td>-0.055170</td>\n      <td>-0.046756</td>\n      <td>-0.704483</td>\n      <td>-0.759990</td>\n      <td>-1.042536</td>\n      <td>-1.143190</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2400</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>2406</td>\n      <td>2021-11-29</td>\n      <td>0.534437</td>\n      <td>-1.136781</td>\n      <td>1.505894</td>\n      <td>-1.190443</td>\n      <td>...</td>\n      <td>1.818819</td>\n      <td>1.031276</td>\n      <td>1.512096</td>\n      <td>-0.824439</td>\n      <td>-0.534328</td>\n      <td>-0.416452</td>\n      <td>2.195698</td>\n      <td>1.699644</td>\n      <td>0.822318</td>\n      <td>0.443818</td>\n    </tr>\n    <tr>\n      <th>2401</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>2407</td>\n      <td>2021-11-30</td>\n      <td>0.915285</td>\n      <td>-1.112226</td>\n      <td>1.404070</td>\n      <td>-1.165418</td>\n      <td>...</td>\n      <td>0.873176</td>\n      <td>1.031276</td>\n      <td>0.815918</td>\n      <td>-0.830065</td>\n      <td>-0.533581</td>\n      <td>-0.409498</td>\n      <td>2.318976</td>\n      <td>1.678364</td>\n      <td>0.533745</td>\n      <td>0.381303</td>\n    </tr>\n    <tr>\n      <th>2402</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>2408</td>\n      <td>2021-12-01</td>\n      <td>0.328554</td>\n      <td>-1.113259</td>\n      <td>1.408238</td>\n      <td>-1.166470</td>\n      <td>...</td>\n      <td>1.054461</td>\n      <td>1.774970</td>\n      <td>0.741040</td>\n      <td>-0.834063</td>\n      <td>-0.556413</td>\n      <td>-0.412365</td>\n      <td>2.346959</td>\n      <td>1.643454</td>\n      <td>0.429537</td>\n      <td>0.580646</td>\n    </tr>\n    <tr>\n      <th>2403</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>2409</td>\n      <td>2021-12-02</td>\n      <td>0.522385</td>\n      <td>-1.113728</td>\n      <td>1.410134</td>\n      <td>-1.166948</td>\n      <td>...</td>\n      <td>1.100556</td>\n      <td>1.774970</td>\n      <td>0.675672</td>\n      <td>-0.838373</td>\n      <td>-0.577372</td>\n      <td>-0.415137</td>\n      <td>2.417174</td>\n      <td>1.576462</td>\n      <td>0.646957</td>\n      <td>0.947160</td>\n    </tr>\n    <tr>\n      <th>2404</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>2410</td>\n      <td>2021-12-03</td>\n      <td>0.143471</td>\n      <td>-1.130964</td>\n      <td>1.481260</td>\n      <td>-1.184515</td>\n      <td>...</td>\n      <td>1.107237</td>\n      <td>1.774970</td>\n      <td>1.013093</td>\n      <td>-0.838613</td>\n      <td>-0.614772</td>\n      <td>-0.421402</td>\n      <td>2.582774</td>\n      <td>1.574862</td>\n      <td>1.046325</td>\n      <td>1.202162</td>\n    </tr>\n  </tbody>\n</table>\n<p>2405 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_in_sample = None\n",
    "for i_month in para.month_in_sample:\n",
    "    file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "    data_curr_month = pd.read_csv(file_name)\n",
    "\n",
    "    data_curr_month = data_curr_month.dropna(axis=0)\n",
    "\n",
    "    data_curr_month.insert(loc=0, column='return_bin', value=np.nan)\n",
    "\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']>0, 'return_bin'] = 0\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']<=0, 'return_bin'] = 1\n",
    "\n",
    "    if i_month == para.month_in_sample[0]:\n",
    "        data_in_sample = data_curr_month\n",
    "    else:\n",
    "        data_in_sample = pd.concat([data_in_sample, data_curr_month])\n",
    "        # data_in_sample = data_in_sample.append(data_curr_month)\n",
    "\n",
    "data_in_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0        -0.327793      0.119714     -0.281492  -0.491825 -0.453595   \n1        -0.308310      0.092927     -0.260658  -0.524533 -0.473787   \n2        -0.340148      0.137024     -0.294705  -0.531742 -0.491461   \n3        -0.352324      0.154334     -0.307724  -0.528374 -0.504890   \n4        -0.407505      0.236085     -0.366732  -0.493942 -0.508368   \n...            ...           ...           ...        ...       ...   \n2159     -1.122708      1.622430     -1.169017   0.203439  0.251474   \n2160     -1.123224      1.624665     -1.169547   0.199552  0.243039   \n2161     -1.128802      1.648983     -1.175277   0.240226  0.244872   \n2162     -1.150504      1.746590     -1.197569   0.463518  0.293449   \n2163     -1.159663      1.789276     -1.206976   0.716304  0.385645   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0     -0.236368 -1.516255  0.287582 -1.499037  0.243383 -0.108178 -0.064657   \n1     -0.284174 -1.570016  0.287582 -1.622616  0.227465 -0.093272 -0.060556   \n2     -0.250539 -1.041819  0.287582 -1.152234  0.209195 -0.100443 -0.063556   \n3     -0.193818 -1.155946  0.287582 -0.927281  0.208047 -0.097007 -0.059870   \n4     -0.060878 -0.648100  1.031276 -0.200881  0.205168 -0.055170 -0.046756   \n...         ...       ...       ...       ...       ...       ...       ...   \n2159  -0.141590  0.015421  0.287582 -0.120539 -0.886344 -1.019756 -0.943319   \n2160  -0.127109  0.311329  0.287582 -0.131706 -0.878593 -1.027028 -0.940690   \n2161   0.010085  0.153422  1.031276 -0.015087 -0.866933 -1.029284 -0.940762   \n2162   0.634608  0.552886  1.031276  0.461300 -0.795231 -1.000395 -0.919978   \n2163   1.217366  0.079326  1.774970  0.650368 -0.807985 -0.975635 -0.901423   \n\n        VOLT20    VOLT60        AR        BR  \n0    -0.714641 -0.765841 -1.286900 -1.005713  \n1    -0.695599 -0.763324 -1.420785 -1.197929  \n2    -0.697842 -0.760783 -1.311010 -1.105942  \n3    -0.700386 -0.759030 -1.307988 -1.151213  \n4    -0.704483 -0.759990 -1.042536 -1.143190  \n...        ...       ...       ...       ...  \n2159  0.095696  0.109752  0.042382  0.741488  \n2160  0.054954  0.072981  0.155255  1.001939  \n2161  0.067985  0.044237 -0.108777  0.737335  \n2162  0.185092  0.044332 -0.080123  0.957555  \n2163  0.413746  0.079081  0.147858  1.251369  \n\n[2164 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.327793</td>\n      <td>0.119714</td>\n      <td>-0.281492</td>\n      <td>-0.491825</td>\n      <td>-0.453595</td>\n      <td>-0.236368</td>\n      <td>-1.516255</td>\n      <td>0.287582</td>\n      <td>-1.499037</td>\n      <td>0.243383</td>\n      <td>-0.108178</td>\n      <td>-0.064657</td>\n      <td>-0.714641</td>\n      <td>-0.765841</td>\n      <td>-1.286900</td>\n      <td>-1.005713</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.308310</td>\n      <td>0.092927</td>\n      <td>-0.260658</td>\n      <td>-0.524533</td>\n      <td>-0.473787</td>\n      <td>-0.284174</td>\n      <td>-1.570016</td>\n      <td>0.287582</td>\n      <td>-1.622616</td>\n      <td>0.227465</td>\n      <td>-0.093272</td>\n      <td>-0.060556</td>\n      <td>-0.695599</td>\n      <td>-0.763324</td>\n      <td>-1.420785</td>\n      <td>-1.197929</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.340148</td>\n      <td>0.137024</td>\n      <td>-0.294705</td>\n      <td>-0.531742</td>\n      <td>-0.491461</td>\n      <td>-0.250539</td>\n      <td>-1.041819</td>\n      <td>0.287582</td>\n      <td>-1.152234</td>\n      <td>0.209195</td>\n      <td>-0.100443</td>\n      <td>-0.063556</td>\n      <td>-0.697842</td>\n      <td>-0.760783</td>\n      <td>-1.311010</td>\n      <td>-1.105942</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.352324</td>\n      <td>0.154334</td>\n      <td>-0.307724</td>\n      <td>-0.528374</td>\n      <td>-0.504890</td>\n      <td>-0.193818</td>\n      <td>-1.155946</td>\n      <td>0.287582</td>\n      <td>-0.927281</td>\n      <td>0.208047</td>\n      <td>-0.097007</td>\n      <td>-0.059870</td>\n      <td>-0.700386</td>\n      <td>-0.759030</td>\n      <td>-1.307988</td>\n      <td>-1.151213</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.407505</td>\n      <td>0.236085</td>\n      <td>-0.366732</td>\n      <td>-0.493942</td>\n      <td>-0.508368</td>\n      <td>-0.060878</td>\n      <td>-0.648100</td>\n      <td>1.031276</td>\n      <td>-0.200881</td>\n      <td>0.205168</td>\n      <td>-0.055170</td>\n      <td>-0.046756</td>\n      <td>-0.704483</td>\n      <td>-0.759990</td>\n      <td>-1.042536</td>\n      <td>-1.143190</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2159</th>\n      <td>-1.122708</td>\n      <td>1.622430</td>\n      <td>-1.169017</td>\n      <td>0.203439</td>\n      <td>0.251474</td>\n      <td>-0.141590</td>\n      <td>0.015421</td>\n      <td>0.287582</td>\n      <td>-0.120539</td>\n      <td>-0.886344</td>\n      <td>-1.019756</td>\n      <td>-0.943319</td>\n      <td>0.095696</td>\n      <td>0.109752</td>\n      <td>0.042382</td>\n      <td>0.741488</td>\n    </tr>\n    <tr>\n      <th>2160</th>\n      <td>-1.123224</td>\n      <td>1.624665</td>\n      <td>-1.169547</td>\n      <td>0.199552</td>\n      <td>0.243039</td>\n      <td>-0.127109</td>\n      <td>0.311329</td>\n      <td>0.287582</td>\n      <td>-0.131706</td>\n      <td>-0.878593</td>\n      <td>-1.027028</td>\n      <td>-0.940690</td>\n      <td>0.054954</td>\n      <td>0.072981</td>\n      <td>0.155255</td>\n      <td>1.001939</td>\n    </tr>\n    <tr>\n      <th>2161</th>\n      <td>-1.128802</td>\n      <td>1.648983</td>\n      <td>-1.175277</td>\n      <td>0.240226</td>\n      <td>0.244872</td>\n      <td>0.010085</td>\n      <td>0.153422</td>\n      <td>1.031276</td>\n      <td>-0.015087</td>\n      <td>-0.866933</td>\n      <td>-1.029284</td>\n      <td>-0.940762</td>\n      <td>0.067985</td>\n      <td>0.044237</td>\n      <td>-0.108777</td>\n      <td>0.737335</td>\n    </tr>\n    <tr>\n      <th>2162</th>\n      <td>-1.150504</td>\n      <td>1.746590</td>\n      <td>-1.197569</td>\n      <td>0.463518</td>\n      <td>0.293449</td>\n      <td>0.634608</td>\n      <td>0.552886</td>\n      <td>1.031276</td>\n      <td>0.461300</td>\n      <td>-0.795231</td>\n      <td>-1.000395</td>\n      <td>-0.919978</td>\n      <td>0.185092</td>\n      <td>0.044332</td>\n      <td>-0.080123</td>\n      <td>0.957555</td>\n    </tr>\n    <tr>\n      <th>2163</th>\n      <td>-1.159663</td>\n      <td>1.789276</td>\n      <td>-1.206976</td>\n      <td>0.716304</td>\n      <td>0.385645</td>\n      <td>1.217366</td>\n      <td>0.079326</td>\n      <td>1.774970</td>\n      <td>0.650368</td>\n      <td>-0.807985</td>\n      <td>-0.975635</td>\n      <td>-0.901423</td>\n      <td>0.413746</td>\n      <td>0.079081</td>\n      <td>0.147858</td>\n      <td>1.251369</td>\n    </tr>\n  </tbody>\n</table>\n<p>2164 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in_sample = data_in_sample.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "y_in_sample = data_in_sample.loc[:, 'return_bin']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False) # True, random_state=para.seed)\n",
    "# X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0       1.0\n1       0.0\n2       1.0\n3       0.0\n4       1.0\n       ... \n2159    0.0\n2160    0.0\n2161    0.0\n2162    0.0\n2163    0.0\nName: return_bin, Length: 2164, dtype: float64"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0        -0.327793      0.119714     -0.281492  -0.491825 -0.453595   \n1        -0.308310      0.092927     -0.260658  -0.524533 -0.473787   \n2        -0.340148      0.137024     -0.294705  -0.531742 -0.491461   \n3        -0.352324      0.154334     -0.307724  -0.528374 -0.504890   \n4        -0.407505      0.236085     -0.366732  -0.493942 -0.508368   \n...            ...           ...           ...        ...       ...   \n2159     -1.122708      1.622430     -1.169017   0.203439  0.251474   \n2160     -1.123224      1.624665     -1.169547   0.199552  0.243039   \n2161     -1.128802      1.648983     -1.175277   0.240226  0.244872   \n2162     -1.150504      1.746590     -1.197569   0.463518  0.293449   \n2163     -1.159663      1.789276     -1.206976   0.716304  0.385645   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0     -0.236368 -1.516255  0.287582 -1.499037  0.243383 -0.108178 -0.064657   \n1     -0.284174 -1.570016  0.287582 -1.622616  0.227465 -0.093272 -0.060556   \n2     -0.250539 -1.041819  0.287582 -1.152234  0.209195 -0.100443 -0.063556   \n3     -0.193818 -1.155946  0.287582 -0.927281  0.208047 -0.097007 -0.059870   \n4     -0.060878 -0.648100  1.031276 -0.200881  0.205168 -0.055170 -0.046756   \n...         ...       ...       ...       ...       ...       ...       ...   \n2159  -0.141590  0.015421  0.287582 -0.120539 -0.886344 -1.019756 -0.943319   \n2160  -0.127109  0.311329  0.287582 -0.131706 -0.878593 -1.027028 -0.940690   \n2161   0.010085  0.153422  1.031276 -0.015087 -0.866933 -1.029284 -0.940762   \n2162   0.634608  0.552886  1.031276  0.461300 -0.795231 -1.000395 -0.919978   \n2163   1.217366  0.079326  1.774970  0.650368 -0.807985 -0.975635 -0.901423   \n\n        VOLT20    VOLT60        AR        BR  return_bin  \n0    -0.714641 -0.765841 -1.286900 -1.005713         1.0  \n1    -0.695599 -0.763324 -1.420785 -1.197929         0.0  \n2    -0.697842 -0.760783 -1.311010 -1.105942         1.0  \n3    -0.700386 -0.759030 -1.307988 -1.151213         0.0  \n4    -0.704483 -0.759990 -1.042536 -1.143190         1.0  \n...        ...       ...       ...       ...         ...  \n2159  0.095696  0.109752  0.042382  0.741488         0.0  \n2160  0.054954  0.072981  0.155255  1.001939         0.0  \n2161  0.067985  0.044237 -0.108777  0.737335         0.0  \n2162  0.185092  0.044332 -0.080123  0.957555         0.0  \n2163  0.413746  0.079081  0.147858  1.251369         0.0  \n\n[2164 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n      <th>return_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.327793</td>\n      <td>0.119714</td>\n      <td>-0.281492</td>\n      <td>-0.491825</td>\n      <td>-0.453595</td>\n      <td>-0.236368</td>\n      <td>-1.516255</td>\n      <td>0.287582</td>\n      <td>-1.499037</td>\n      <td>0.243383</td>\n      <td>-0.108178</td>\n      <td>-0.064657</td>\n      <td>-0.714641</td>\n      <td>-0.765841</td>\n      <td>-1.286900</td>\n      <td>-1.005713</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.308310</td>\n      <td>0.092927</td>\n      <td>-0.260658</td>\n      <td>-0.524533</td>\n      <td>-0.473787</td>\n      <td>-0.284174</td>\n      <td>-1.570016</td>\n      <td>0.287582</td>\n      <td>-1.622616</td>\n      <td>0.227465</td>\n      <td>-0.093272</td>\n      <td>-0.060556</td>\n      <td>-0.695599</td>\n      <td>-0.763324</td>\n      <td>-1.420785</td>\n      <td>-1.197929</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.340148</td>\n      <td>0.137024</td>\n      <td>-0.294705</td>\n      <td>-0.531742</td>\n      <td>-0.491461</td>\n      <td>-0.250539</td>\n      <td>-1.041819</td>\n      <td>0.287582</td>\n      <td>-1.152234</td>\n      <td>0.209195</td>\n      <td>-0.100443</td>\n      <td>-0.063556</td>\n      <td>-0.697842</td>\n      <td>-0.760783</td>\n      <td>-1.311010</td>\n      <td>-1.105942</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.352324</td>\n      <td>0.154334</td>\n      <td>-0.307724</td>\n      <td>-0.528374</td>\n      <td>-0.504890</td>\n      <td>-0.193818</td>\n      <td>-1.155946</td>\n      <td>0.287582</td>\n      <td>-0.927281</td>\n      <td>0.208047</td>\n      <td>-0.097007</td>\n      <td>-0.059870</td>\n      <td>-0.700386</td>\n      <td>-0.759030</td>\n      <td>-1.307988</td>\n      <td>-1.151213</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.407505</td>\n      <td>0.236085</td>\n      <td>-0.366732</td>\n      <td>-0.493942</td>\n      <td>-0.508368</td>\n      <td>-0.060878</td>\n      <td>-0.648100</td>\n      <td>1.031276</td>\n      <td>-0.200881</td>\n      <td>0.205168</td>\n      <td>-0.055170</td>\n      <td>-0.046756</td>\n      <td>-0.704483</td>\n      <td>-0.759990</td>\n      <td>-1.042536</td>\n      <td>-1.143190</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2159</th>\n      <td>-1.122708</td>\n      <td>1.622430</td>\n      <td>-1.169017</td>\n      <td>0.203439</td>\n      <td>0.251474</td>\n      <td>-0.141590</td>\n      <td>0.015421</td>\n      <td>0.287582</td>\n      <td>-0.120539</td>\n      <td>-0.886344</td>\n      <td>-1.019756</td>\n      <td>-0.943319</td>\n      <td>0.095696</td>\n      <td>0.109752</td>\n      <td>0.042382</td>\n      <td>0.741488</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2160</th>\n      <td>-1.123224</td>\n      <td>1.624665</td>\n      <td>-1.169547</td>\n      <td>0.199552</td>\n      <td>0.243039</td>\n      <td>-0.127109</td>\n      <td>0.311329</td>\n      <td>0.287582</td>\n      <td>-0.131706</td>\n      <td>-0.878593</td>\n      <td>-1.027028</td>\n      <td>-0.940690</td>\n      <td>0.054954</td>\n      <td>0.072981</td>\n      <td>0.155255</td>\n      <td>1.001939</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2161</th>\n      <td>-1.128802</td>\n      <td>1.648983</td>\n      <td>-1.175277</td>\n      <td>0.240226</td>\n      <td>0.244872</td>\n      <td>0.010085</td>\n      <td>0.153422</td>\n      <td>1.031276</td>\n      <td>-0.015087</td>\n      <td>-0.866933</td>\n      <td>-1.029284</td>\n      <td>-0.940762</td>\n      <td>0.067985</td>\n      <td>0.044237</td>\n      <td>-0.108777</td>\n      <td>0.737335</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2162</th>\n      <td>-1.150504</td>\n      <td>1.746590</td>\n      <td>-1.197569</td>\n      <td>0.463518</td>\n      <td>0.293449</td>\n      <td>0.634608</td>\n      <td>0.552886</td>\n      <td>1.031276</td>\n      <td>0.461300</td>\n      <td>-0.795231</td>\n      <td>-1.000395</td>\n      <td>-0.919978</td>\n      <td>0.185092</td>\n      <td>0.044332</td>\n      <td>-0.080123</td>\n      <td>0.957555</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2163</th>\n      <td>-1.159663</td>\n      <td>1.789276</td>\n      <td>-1.206976</td>\n      <td>0.716304</td>\n      <td>0.385645</td>\n      <td>1.217366</td>\n      <td>0.079326</td>\n      <td>1.774970</td>\n      <td>0.650368</td>\n      <td>-0.807985</td>\n      <td>-0.975635</td>\n      <td>-0.901423</td>\n      <td>0.413746</td>\n      <td>0.079081</td>\n      <td>0.147858</td>\n      <td>1.251369</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2164 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_cv = pd.concat([X_cv, y_cv], axis=1)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "X_train_ndarray = X_train.values\n",
    "y_train_ndarray = y_train.values\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_ndarray).type(torch.FloatTensor), torch.from_numpy(y_train_ndarray).type(torch.LongTensor))\n",
    "\n",
    "# for X_train_temp, y_train_temp in train_dataset:\n",
    "#     print(X_train_temp, y_train_temp)\n",
    "#     print(X_train_temp.dtype, y_train_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_cv_ndarray = X_cv.values\n",
    "y_cv_ndarray = y_cv.values\n",
    "\n",
    "cv_dataset = TensorDataset(torch.from_numpy(X_cv_ndarray).type(torch.FloatTensor), torch.from_numpy(y_cv.values).type(torch.LongTensor))\n",
    "\n",
    "# for X_cv_temp, y_cv_temp in cv_dataset:\n",
    "#     print(X_cv_temp, y_cv_temp)\n",
    "#     print(X_cv_temp.dtype, y_cv_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cv_dataloader = DataLoader(\n",
    "    dataset=cv_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_test = None\n",
    "# for i_month in para.month_test:\n",
    "#\n",
    "#     file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "#     data_curr_month = pd.read_csv(file_name)\n",
    "#\n",
    "#     data_curr_month = data_curr_month.dropna(axis=0)\n",
    "#\n",
    "#     data_curr_month = label_data(data=data_curr_month, percent_select=para.percent_select)\n",
    "#\n",
    "#     if i_month == para.month_test[0]:\n",
    "#         data_test = data_curr_month\n",
    "#     else:\n",
    "#         data_test = pd.concat([data_test, data_curr_month])\n",
    "#         # data_test = data_test.append(data_curr_month)\n",
    "#\n",
    "# X_test = data_test.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "# y_test = data_test.loc[:, 'return_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset\n",
    "# import torch\n",
    "#\n",
    "#\n",
    "# X_test_ndarray = X_test.values\n",
    "# y_test_ndarray = y_test.values\n",
    "#\n",
    "# test_dataset = TensorDataset(torch.from_numpy(X_test_ndarray).type(torch.FloatTensor), torch.from_numpy(y_test_ndarray).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "#\n",
    "#\n",
    "# test_dataloader = DataLoader(\n",
    "#     dataset=test_dataset,\n",
    "#     batch_size=para.batch_size,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from my_utils.model_class import MLP\n",
    "\n",
    "model = MLP(in_nums=len(X_train.columns), out_nums=para.classification, drop_p=para.drop)\n",
    "# to device\n",
    "model = model.to(device=para.device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    train_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # to device\n",
    "        X = X.to(device=para.device)\n",
    "        y = y.to(device=para.device)\n",
    "\n",
    "        # compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        train_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # metric on current batch\n",
    "        train_precision(pred.argmax(1), y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = train_precision.compute()\n",
    "    print(\"Precision of every train dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, train_loss, total_precision\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    test_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            # to device\n",
    "            X = X.to(device=para.device)\n",
    "            y = y.to(device=para.device)\n",
    "\n",
    "            # compute prediction and loss\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            # metric on current batch\n",
    "            test_precision(pred.argmax(1), y)\n",
    "\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = test_precision.compute()\n",
    "    print(\"Precision of every test dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, test_loss, total_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_df_to_dataloader(df: pd.DataFrame, select: int) -> DataLoader:\n",
    "\n",
    "    df = df[df['return_bin'] == select]\n",
    "\n",
    "    df_dataset = TensorDataset(\n",
    "        torch.from_numpy(df.loc[:, para.feature_column_start_name: para.feature_column_end_name].values).type(torch.FloatTensor),\n",
    "        torch.from_numpy(df.loc[:, 'return_bin'].values).type(torch.LongTensor))\n",
    "\n",
    "    df_dataloader = DataLoader(\n",
    "        dataset=df_dataset,\n",
    "        batch_size=para.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return df_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp2_dataloader = select_df_to_dataloader(df=data_cv, select=2)\n",
    "temp1_dataloader = select_df_to_dataloader(df=data_cv, select=1)\n",
    "temp0_dataloader = select_df_to_dataloader(df=data_cv, select=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 50.7%, Avg loss: 0.695929 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.4994, 0.5128], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.9%, Avg loss: 0.688260 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6667, 0.5769], device='cuda:0')\n",
      "\n",
      "Time cost = 1.773974s\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 51.3%, Avg loss: 0.693084 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5093, 0.5151], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 53.1%, Avg loss: 0.684278 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.4286, 0.5409], device='cuda:0')\n",
      "\n",
      "Time cost = 3.391373s\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 53.5%, Avg loss: 0.685561 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5448, 0.5303], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 59.3%, Avg loss: 0.685718 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5769, 0.6012], device='cuda:0')\n",
      "\n",
      "Time cost = 4.886712s\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 53.1%, Avg loss: 0.687598 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5321, 0.5303], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 56.8%, Avg loss: 0.678994 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5625, 0.5699], device='cuda:0')\n",
      "\n",
      "Time cost = 6.398456s\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 54.3%, Avg loss: 0.686226 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5469, 0.5399], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.7%, Avg loss: 0.681273 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6316, 0.6242], device='cuda:0')\n",
      "\n",
      "Time cost = 7.680361s\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 55.9%, Avg loss: 0.682555 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5580, 0.5601], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 61.0%, Avg loss: 0.673218 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6212, 0.6057], device='cuda:0')\n",
      "\n",
      "Time cost = 9.026343s\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 54.6%, Avg loss: 0.685685 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5466, 0.5460], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.2%, Avg loss: 0.678060 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6267, 0.6205], device='cuda:0')\n",
      "\n",
      "Time cost = 10.338212s\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 55.7%, Avg loss: 0.681056 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5544, 0.5597], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 63.9%, Avg loss: 0.684705 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6620, 0.6294], device='cuda:0')\n",
      "\n",
      "Time cost = 11.838366s\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.4%, Avg loss: 0.679563 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5577, 0.5705], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 63.1%, Avg loss: 0.673087 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6438, 0.6250], device='cuda:0')\n",
      "\n",
      "Time cost = 13.084932s\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 55.7%, Avg loss: 0.680167 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5509, 0.5624], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.2%, Avg loss: 0.652475 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6727, 0.6075], device='cuda:0')\n",
      "\n",
      "Time cost = 14.493876s\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.2%, Avg loss: 0.678031 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5618, 0.5628], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.7%, Avg loss: 0.655560 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6923, 0.6085], device='cuda:0')\n",
      "\n",
      "Time cost = 15.770430s\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.8%, Avg loss: 0.674979 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5742, 0.5806], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 63.1%, Avg loss: 0.671973 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6780, 0.6154], device='cuda:0')\n",
      "\n",
      "Time cost = 16.969574s\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.4%, Avg loss: 0.673852 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5703, 0.5781], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.7%, Avg loss: 0.670024 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6712, 0.6369], device='cuda:0')\n",
      "\n",
      "Time cost = 18.075095s\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.9%, Avg loss: 0.672470 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5673, 0.5701], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 66.8%, Avg loss: 0.647948 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7206, 0.6474], device='cuda:0')\n",
      "\n",
      "Time cost = 19.194493s\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.1%, Avg loss: 0.671181 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5813, 0.6023], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 66.8%, Avg loss: 0.651040 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7885, 0.6349], device='cuda:0')\n",
      "\n",
      "Time cost = 20.535219s\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.9%, Avg loss: 0.671597 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5722, 0.5846], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 63.9%, Avg loss: 0.672797 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7805, 0.6100], device='cuda:0')\n",
      "\n",
      "Time cost = 21.671676s\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.1%, Avg loss: 0.660851 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5910, 0.5902], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.6%, Avg loss: 0.649446 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8000, 0.6224], device='cuda:0')\n",
      "\n",
      "Time cost = 22.762070s\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 58.1%, Avg loss: 0.664285 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5748, 0.5876], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.3%, Avg loss: 0.656597 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8000, 0.6119], device='cuda:0')\n",
      "\n",
      "Time cost = 23.857778s\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.4%, Avg loss: 0.662146 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5843, 0.6047], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 61.4%, Avg loss: 0.661625 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7742, 0.5905], device='cuda:0')\n",
      "\n",
      "Time cost = 24.964487s\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.8%, Avg loss: 0.662714 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5936, 0.6011], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.7%, Avg loss: 0.661514 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7273, 0.6041], device='cuda:0')\n",
      "\n",
      "Time cost = 26.128957s\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.2%, Avg loss: 0.661874 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5994, 0.6037], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 63.1%, Avg loss: 0.635269 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7333, 0.6071], device='cuda:0')\n",
      "\n",
      "Time cost = 27.236172s\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.1%, Avg loss: 0.658986 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5834, 0.5996], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 61.8%, Avg loss: 0.668571 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7250, 0.5970], device='cuda:0')\n",
      "\n",
      "Time cost = 28.374970s\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.2%, Avg loss: 0.656892 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6045, 0.5993], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 61.4%, Avg loss: 0.650364 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7429, 0.5922], device='cuda:0')\n",
      "\n",
      "Time cost = 29.470328s\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.1%, Avg loss: 0.652623 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5787, 0.6049], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 61.4%, Avg loss: 0.658218 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7931, 0.5896], device='cuda:0')\n",
      "\n",
      "Time cost = 30.569433s\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.7%, Avg loss: 0.657978 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6112, 0.6040], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 60.2%, Avg loss: 0.642876 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6842, 0.5862], device='cuda:0')\n",
      "\n",
      "Time cost = 31.755655s\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.5%, Avg loss: 0.652481 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6078, 0.6025], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 60.6%, Avg loss: 0.640869 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7027, 0.5882], device='cuda:0')\n",
      "\n",
      "Time cost = 32.860868s\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.2%, Avg loss: 0.655961 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6192, 0.6069], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 59.8%, Avg loss: 0.644655 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8095, 0.5773], device='cuda:0')\n",
      "\n",
      "Time cost = 33.934189s\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.2%, Avg loss: 0.655991 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6201, 0.6055], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.2%, Avg loss: 0.655202 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7436, 0.5990], device='cuda:0')\n",
      "\n",
      "Time cost = 35.025000s\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.2%, Avg loss: 0.646139 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6394, 0.6264], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 60.2%, Avg loss: 0.642304 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6400, 0.5916], device='cuda:0')\n",
      "\n",
      "Time cost = 36.102831s\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.9%, Avg loss: 0.653614 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6257, 0.6141], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 61.4%, Avg loss: 0.647515 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8696, 0.5872], device='cuda:0')\n",
      "\n",
      "Time cost = 37.261306s\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.1%, Avg loss: 0.653238 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6137, 0.6086], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.3%, Avg loss: 0.645597 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8158, 0.6108], device='cuda:0')\n",
      "\n",
      "Time cost = 38.354782s\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.1%, Avg loss: 0.642522 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6229, 0.6195], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 61.8%, Avg loss: 0.649756 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7812, 0.5933], device='cuda:0')\n",
      "\n",
      "Time cost = 39.419814s\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.8%, Avg loss: 0.652182 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6241, 0.6137], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.7%, Avg loss: 0.655807 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7174, 0.6051], device='cuda:0')\n",
      "\n",
      "Time cost = 40.591768s\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.8%, Avg loss: 0.645448 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6281, 0.6288], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.3%, Avg loss: 0.694295 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8529, 0.6087], device='cuda:0')\n",
      "\n",
      "Time cost = 41.835102s\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.6%, Avg loss: 0.641942 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6382, 0.6171], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.1%, Avg loss: 0.713501 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8824, 0.6135], device='cuda:0')\n",
      "\n",
      "Time cost = 43.219168s\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.8%, Avg loss: 0.653220 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6372, 0.6216], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.7%, Avg loss: 0.633131 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7358, 0.6223], device='cuda:0')\n",
      "\n",
      "Time cost = 44.539113s\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.8%, Avg loss: 0.644571 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6362, 0.6208], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.1%, Avg loss: 0.636643 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8095, 0.6181], device='cuda:0')\n",
      "\n",
      "Time cost = 45.677504s\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.5%, Avg loss: 0.630829 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6456, 0.6275], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.7%, Avg loss: 0.647980 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7119, 0.6264], device='cuda:0')\n",
      "\n",
      "Time cost = 46.804628s\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.0%, Avg loss: 0.640272 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6379, 0.6243], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 66.4%, Avg loss: 0.623912 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7843, 0.6316], device='cuda:0')\n",
      "\n",
      "Time cost = 47.994262s\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.2%, Avg loss: 0.642635 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6200, 0.6064], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.6%, Avg loss: 0.636287 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8636, 0.6345], device='cuda:0')\n",
      "\n",
      "Time cost = 49.094095s\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.2%, Avg loss: 0.645716 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6350, 0.6125], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.6%, Avg loss: 0.618659 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9211, 0.6305], device='cuda:0')\n",
      "\n",
      "Time cost = 50.214873s\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.1%, Avg loss: 0.633643 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6503, 0.6335], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 66.8%, Avg loss: 0.635487 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7885, 0.6349], device='cuda:0')\n",
      "\n",
      "Time cost = 51.419159s\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.4%, Avg loss: 0.632402 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6389, 0.6300], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.7%, Avg loss: 0.658032 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9310, 0.6085], device='cuda:0')\n",
      "\n",
      "Time cost = 52.640079s\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.1%, Avg loss: 0.633475 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6465, 0.6201], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.6%, Avg loss: 0.740476 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9706, 0.6280], device='cuda:0')\n",
      "\n",
      "Time cost = 53.860837s\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.9%, Avg loss: 0.637480 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6599, 0.6246], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.9%, Avg loss: 0.654722 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9487, 0.6386], device='cuda:0')\n",
      "\n",
      "Time cost = 55.177231s\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.9%, Avg loss: 0.640942 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6406, 0.6201], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.7%, Avg loss: 0.617250 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6065], device='cuda:0')\n",
      "\n",
      "Time cost = 56.524787s\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.8%, Avg loss: 0.638889 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6485, 0.6148], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.1%, Avg loss: 0.636967 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9643, 0.6103], device='cuda:0')\n",
      "\n",
      "Time cost = 57.733527s\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.7%, Avg loss: 0.639264 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6493, 0.6123], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 66.8%, Avg loss: 0.638466 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9167, 0.6244], device='cuda:0')\n",
      "\n",
      "Time cost = 59.060779s\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.9%, Avg loss: 0.639408 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6466, 0.6172], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 70.5%, Avg loss: 0.636798 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8980, 0.6562], device='cuda:0')\n",
      "\n",
      "Time cost = 60.296646s\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.9%, Avg loss: 0.636554 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6595, 0.6241], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 70.1%, Avg loss: 0.607685 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8654, 0.6561], device='cuda:0')\n",
      "\n",
      "Time cost = 61.597873s\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.0%, Avg loss: 0.629510 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6551, 0.6281], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.9%, Avg loss: 0.619510 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7692, 0.6591], device='cuda:0')\n",
      "\n",
      "Time cost = 62.830854s\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.4%, Avg loss: 0.627245 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6553, 0.6201], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.7%, Avg loss: 0.603378 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9111, 0.6480], device='cuda:0')\n",
      "\n",
      "Time cost = 64.109699s\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.4%, Avg loss: 0.633011 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6611, 0.6159], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 70.5%, Avg loss: 0.611784 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9535, 0.6515], device='cuda:0')\n",
      "\n",
      "Time cost = 65.340419s\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.2%, Avg loss: 0.638969 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6555, 0.6166], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 63.9%, Avg loss: 0.640349 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6009], device='cuda:0')\n",
      "\n",
      "Time cost = 66.588460s\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.8%, Avg loss: 0.627426 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6699, 0.6178], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.6%, Avg loss: 0.637773 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6268], device='cuda:0')\n",
      "\n",
      "Time cost = 67.709860s\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.8%, Avg loss: 0.640087 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6455, 0.6164], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.7%, Avg loss: 0.614835 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9744, 0.6436], device='cuda:0')\n",
      "\n",
      "Time cost = 68.829427s\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.0%, Avg loss: 0.635081 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6546, 0.6144], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.6%, Avg loss: 0.623268 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6121], device='cuda:0')\n",
      "\n",
      "Time cost = 70.047682s\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.9%, Avg loss: 0.629956 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6610, 0.6233], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.5%, Avg loss: 0.628757 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9722, 0.6341], device='cuda:0')\n",
      "\n",
      "Time cost = 71.143603s\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.7%, Avg loss: 0.619116 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6724, 0.6304], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.3%, Avg loss: 0.640879 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9737, 0.6404], device='cuda:0')\n",
      "\n",
      "Time cost = 72.248621s\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.1%, Avg loss: 0.631231 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6625, 0.6260], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.9%, Avg loss: 0.665439 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8889, 0.6429], device='cuda:0')\n",
      "\n",
      "Time cost = 73.405266s\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 计时\n",
    "time_start = time.time()\n",
    "\n",
    "# writer = SummaryWriter(para.tensor_board_log_dir)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# to device\n",
    "loss_fn = loss_fn.to(device=para.device)\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=para.lr)\n",
    "\n",
    "epochs = para.epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    model.train()\n",
    "    accuracy_train, loss_train, precision_train = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    model.eval()\n",
    "    accuracy_cv, loss_cv, precision_cv = test_loop(cv_dataloader, model, loss_fn)\n",
    "\n",
    "    # accuracy2 = test_loop(temp2_dataloader, model, loss_fn)\n",
    "    # print('#')\n",
    "    # accuracy1 = test_loop(temp1_dataloader, model, loss_fn)\n",
    "    # accuracy0 = test_loop(temp0_dataloader, model, loss_fn)\n",
    "\n",
    "    # 写入 tensorboard\n",
    "    if para.classification == 2:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 3:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 5:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2],\n",
    "                               'precision3': precision_cv[3],\n",
    "                               'precision4': precision_cv[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2],\n",
    "                               'precision3': precision_train[3],\n",
    "                               'precision4': precision_train[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/cv',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_cv},\n",
    "                       global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/train',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_train},\n",
    "                       global_step=t)\n",
    "    writer.flush()\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('Time cost = %fs' % (time_end - time_start))\n",
    "    print()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 保存模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish save model!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), para.save_model_path)\n",
    "\n",
    "print('Finish save model!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # captum\n",
    "# from captum.attr import IntegratedGradients\n",
    "#\n",
    "# ig = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp = cv_dataloader.dataset.tensors[0]\n",
    "# temp.requires_grad_()\n",
    "# attr, delta = ig.attribute(temp,target=1, return_convergence_delta=True)\n",
    "# attr = attr.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Helper method to print importances and visualize distribution\n",
    "# def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "#     print(title)\n",
    "#     for i in range(len(feature_names)):\n",
    "#         print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "#     y_pos = (np.arange(len(feature_names)))\n",
    "#     if plot:\n",
    "#         plt.figure(figsize=(20,6))\n",
    "#         plt.barh(y_pos, importances, align='center')\n",
    "#         plt.yticks(y_pos, feature_names)\n",
    "#         plt.ylabel(axis_title)\n",
    "#         plt.grid(axis='y')\n",
    "#         plt.title(title)\n",
    "# visualize_importances(feature_names=X_cv.columns.values.tolist(), importances=np.mean(attr, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_cv.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.Tensor(\n",
    "#     [[-0.0441,  0.0773],\n",
    "#     [-0.0781, -0.1772],\n",
    "#     [-0.1319, -0.0432],\n",
    "#     [-0.0714, -0.1261],\n",
    "#     [-0.0806, -0.1370],\n",
    "#     [-0.1730, -0.1472],\n",
    "#     [-0.0350, -0.0507],\n",
    "#     [-0.1149, -0.2248]])\n",
    "# # input = input.reshape(-1,4)\n",
    "# target = torch.Tensor([0, 1, 1, 0, 0, 0, 0, 0]).type(torch.LongTensor)\n",
    "# print(input.dtype)\n",
    "# print(target.dtype)\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Example of target with class indices\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.Tensor([1,4,1]).type(torch.LongTensor)\n",
    "# output = loss(input, target)\n",
    "# print(input,target,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# input = torch.Tensor([0.5, 0.4, 0.3])\n",
    "# target = torch.Tensor([0])\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}